{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingestion Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You'll need to install the following libraries if they are not already installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install elasticsearch sentence-transformers pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Elasticsearch client setup using cloud configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_client_es():\n",
    "    \"\"\"\n",
    "    Initializes Elasticsearch client using cloud_id and api_key from config.yml\n",
    "    \"\"\"\n",
    "    with open(\"../config.yml\", \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return Elasticsearch(config[\"cloud_url\"], api_key=config[\"api_key\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Text Vectorization using SentenceTransformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_vector(sentences):\n",
    "    \"\"\"\n",
    "    Generates sentence embeddings using pre-trained model 'all-MiniLM-L6-v2'.\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    embeddings = model.encode(sentences)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Read JSON file containing the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads and loads the dataset from a JSON file.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Chunk data for batch processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(data, batch_size):\n",
    "    \"\"\"\n",
    "    Yields chunks of data in batch sizes for bulk indexing in Elasticsearch.\n",
    "    \"\"\"\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data[i : i + batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Generate bulk actions for Elasticsearch indexing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bulk_actions(index_name, data_batch):\n",
    "    \"\"\"\n",
    "    Generates bulk actions for Elasticsearch from data batches.\n",
    "    Adds 'description_embeddings' by encoding the 'description' field.\n",
    "    \"\"\"\n",
    "    for item in data_batch:\n",
    "        document_id = item[\"id\"]\n",
    "        # item[\"description_embeddings\"] = get_text_vector(item[\"description\"])\n",
    "        yield {\"_index\": index_name, \"_id\": document_id, \"_source\": item}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Indexing data in batches to Elasticsearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_data_in_batches(file_path, index_name, batch_size=100):\n",
    "    \"\"\"\n",
    "    Indexes data from the JSON file in batches using Elasticsearch helpers.bulk.\n",
    "    \"\"\"\n",
    "    data = read_json_file(file_path)\n",
    "\n",
    "    for batch in chunk_data(data, batch_size):\n",
    "        actions = generate_bulk_actions(index_name, batch)\n",
    "        success, failed = helpers.bulk(get_client_es(), actions)\n",
    "        print(f\"Batch indexed: {success} successful, {failed} failed\")\n",
    "\n",
    "\n",
    "# main execution block\n",
    "# if __name__ == '__main__':\n",
    "#     index_data_in_batches(\"../files/dataset/products.json\", \"products-catalog\", batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch indexed: 100 successful, [] failed\n",
      "Batch indexed: 100 successful, [] failed\n",
      "Batch indexed: 100 successful, [] failed\n",
      "Batch indexed: 100 successful, [] failed\n",
      "Batch indexed: 100 successful, [] failed\n",
      "Batch indexed: 100 successful, [] failed\n",
      "Batch indexed: 100 successful, [] failed\n",
      "Batch indexed: 100 successful, [] failed\n",
      "Batch indexed: 100 successful, [] failed\n",
      "Batch indexed: 31 successful, [] failed\n"
     ]
    }
   ],
   "source": [
    "index_data_in_batches(\n",
    "    \"../files/dataset/products.json\", \"products-catalog-2\", batch_size=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"products-catalog-2\"\n",
    "mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"id\": {\"type\": \"keyword\"},\n",
    "            \"brand\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fields\": {\"keyword\": {\"type\": \"keyword\"}},\n",
    "            },\n",
    "            \"name\": {\"type\": \"text\"},\n",
    "            \"price\": {\"type\": \"float\"},\n",
    "            \"price_sign\": {\"type\": \"keyword\"},\n",
    "            \"currency\": {\"type\": \"keyword\"},\n",
    "            \"image_link\": {\"type\": \"keyword\"},\n",
    "            \"description\": {\"type\": \"text\"},\n",
    "            \"description_embeddings\": {\"type\": \"dense_vector\", \"dims\": 384},\n",
    "            \"rating\": {\"type\": \"keyword\"},\n",
    "            \"category\": {\"type\": \"keyword\"},\n",
    "            \"product_type\": {\"type\": \"keyword\"},\n",
    "            \"tag_list\": {\"type\": \"keyword\"},\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "def create_index(index_name, mapping):\n",
    "    if not get_client_es().indices.exists(index=index_name):\n",
    "        get_client_es().indices.create(index=index_name, body=mapping)\n",
    "        print(f\"Index '{index_name}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Index '{index_name}' already exists.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'products-catalog-2' created successfully.\n"
     ]
    }
   ],
   "source": [
    "create_index(index_name, mapping)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
